{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cdb1e6-b190-488b-b216-fb521d905519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c3840d0-ad6d-430e-bdf1-f5ae64cd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_normal = transforms.Compose(\n",
    "    [transforms.Resize((32,32)),\n",
    "     transforms.ToTensor()]\n",
    ")\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32,32)),\n",
    "     transforms.RandomHorizontalFlip(p=1.0), # randomly flip the image horizontally\n",
    "     transforms.ToTensor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eab1fc6c-56ef-4aa5-a55f-297da72ebc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset\n",
    "original_dataset = torchvision.datasets.ImageFolder(root='./dataset', transform=transform_normal)\n",
    "\n",
    "# Apply the transforms to the dataset to create a new augmented dataset\n",
    "augmented_dataset = torchvision.datasets.ImageFolder(root='./dataset', transform=transform)\n",
    "# Combine the original and augmented datasets into a single dataset\n",
    "dataset = torch.utils.data.ConcatDataset([original_dataset, augmented_dataset])\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a357607d-16e6-4675-9ada-d790930770b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  tensor([0.8444, 0.8403, 0.8390])\n",
      "Std:  tensor([0.2176, 0.2225, 0.2245])\n"
     ]
    }
   ],
   "source": [
    "train_mean = 0.0\n",
    "train_std = 0.0\n",
    "for images, _ in trainloader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    train_mean += images.mean(2).sum(0)\n",
    "    train_std += images.std(2).sum(0)\n",
    "\n",
    "train_mean /= len(trainloader.dataset)\n",
    "train_std /= len(trainloader.dataset)\n",
    "\n",
    "print(\"Mean: \", train_mean)\n",
    "print(\"Std: \", train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0407e077-3d5f-4e73-a5d3-78bd22b18fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=train_mean, std=train_std)\n",
    "\n",
    "transform_normal = transforms.Compose(\n",
    "    [transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     normalize]\n",
    ")\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32,32)),\n",
    "     transforms.RandomHorizontalFlip(p=1.0), # randomly flip the image horizontally\n",
    "     transforms.ToTensor(),\n",
    "     normalize]\n",
    ")\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = torchvision.datasets.ImageFolder(root='./dataset', transform=transform_normal)\n",
    "\n",
    "# Apply the transforms to the dataset to create a new augmented dataset\n",
    "augmented_dataset = torchvision.datasets.ImageFolder(root='./dataset', transform=transform)\n",
    "# Combine the original and augmented datasets into a single dataset\n",
    "dataset = torch.utils.data.ConcatDataset([original_dataset, augmented_dataset])\n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa37a9e8-17e3-45ca-a36c-4ea4c58fd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "80266590-63da-400a-91c6-cdfded22090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FurnitureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FurnitureNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = FurnitureNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d217104-9a59-4b43-a53b-2e1ed4680ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7bda2521-4d24-4b68-b964-661853f941f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.493\n",
      "[1,    40] loss: 0.262\n",
      "[1,    60] loss: 0.294\n",
      "[1,    80] loss: 0.118\n",
      "[1,   100] loss: 0.191\n",
      "[1,   120] loss: 0.141\n",
      "Train Accuracy for epoch 0 = 0.7958333333333333\n",
      "Test Accuracy for epoch 0 = 0.9166666666666666\n",
      "[2,    20] loss: 0.112\n",
      "[2,    40] loss: 0.048\n",
      "[2,    60] loss: 0.077\n",
      "[2,    80] loss: 0.155\n",
      "[2,   100] loss: 0.171\n",
      "[2,   120] loss: 0.089\n",
      "Train Accuracy for epoch 1 = 0.9333333333333333\n",
      "Test Accuracy for epoch 1 = 0.925\n",
      "[3,    20] loss: 0.066\n",
      "[3,    40] loss: 0.044\n",
      "[3,    60] loss: 0.036\n",
      "[3,    80] loss: 0.128\n",
      "[3,   100] loss: 0.096\n",
      "[3,   120] loss: 0.074\n",
      "Train Accuracy for epoch 2 = 0.95625\n",
      "Test Accuracy for epoch 2 = 0.9333333333333333\n",
      "[4,    20] loss: 0.037\n",
      "[4,    40] loss: 0.030\n",
      "[4,    60] loss: 0.087\n",
      "[4,    80] loss: 0.041\n",
      "[4,   100] loss: 0.061\n",
      "[4,   120] loss: 0.022\n",
      "Train Accuracy for epoch 3 = 0.9645833333333333\n",
      "Test Accuracy for epoch 3 = 0.9333333333333333\n",
      "[5,    20] loss: 0.062\n",
      "[5,    40] loss: 0.032\n",
      "[5,    60] loss: 0.037\n",
      "[5,    80] loss: 0.018\n",
      "[5,   100] loss: 0.095\n",
      "[5,   120] loss: 0.025\n",
      "Train Accuracy for epoch 4 = 0.96875\n",
      "Test Accuracy for epoch 4 = 0.95\n",
      "[6,    20] loss: 0.017\n",
      "[6,    40] loss: 0.027\n",
      "[6,    60] loss: 0.023\n",
      "[6,    80] loss: 0.038\n",
      "[6,   100] loss: 0.049\n",
      "[6,   120] loss: 0.021\n",
      "Train Accuracy for epoch 5 = 0.9791666666666666\n",
      "Test Accuracy for epoch 5 = 0.9416666666666667\n",
      "[7,    20] loss: 0.031\n",
      "[7,    40] loss: 0.014\n",
      "[7,    60] loss: 0.029\n",
      "[7,    80] loss: 0.130\n",
      "[7,   100] loss: 0.057\n",
      "[7,   120] loss: 0.089\n",
      "Train Accuracy for epoch 6 = 0.95625\n",
      "Test Accuracy for epoch 6 = 0.9416666666666667\n",
      "[8,    20] loss: 0.037\n",
      "[8,    40] loss: 0.022\n",
      "[8,    60] loss: 0.017\n",
      "[8,    80] loss: 0.030\n",
      "[8,   100] loss: 0.035\n",
      "[8,   120] loss: 0.019\n",
      "Train Accuracy for epoch 7 = 0.9854166666666667\n",
      "Test Accuracy for epoch 7 = 0.9416666666666667\n",
      "[9,    20] loss: 0.009\n",
      "[9,    40] loss: 0.004\n",
      "[9,    60] loss: 0.010\n",
      "[9,    80] loss: 0.011\n",
      "[9,   100] loss: 0.013\n",
      "[9,   120] loss: 0.028\n",
      "Train Accuracy for epoch 8 = 0.99375\n",
      "Test Accuracy for epoch 8 = 0.9166666666666666\n",
      "[10,    20] loss: 0.004\n",
      "[10,    40] loss: 0.001\n",
      "[10,    60] loss: 0.013\n",
      "[10,    80] loss: 0.005\n",
      "[10,   100] loss: 0.006\n",
      "[10,   120] loss: 0.001\n",
      "Train Accuracy for epoch 9 = 0.9979166666666667\n",
      "Test Accuracy for epoch 9 = 0.95\n",
      "[11,    20] loss: 0.002\n",
      "[11,    40] loss: 0.022\n",
      "[11,    60] loss: 0.009\n",
      "[11,    80] loss: 0.009\n",
      "[11,   100] loss: 0.001\n",
      "[11,   120] loss: 0.023\n",
      "Train Accuracy for epoch 10 = 0.99375\n",
      "Test Accuracy for epoch 10 = 0.95\n",
      "[12,    20] loss: 0.010\n",
      "[12,    40] loss: 0.003\n",
      "[12,    60] loss: 0.000\n",
      "[12,    80] loss: 0.003\n",
      "[12,   100] loss: 0.026\n",
      "[12,   120] loss: 0.007\n",
      "Train Accuracy for epoch 11 = 0.9979166666666667\n",
      "Test Accuracy for epoch 11 = 0.9333333333333333\n",
      "[13,    20] loss: 0.005\n",
      "[13,    40] loss: 0.001\n",
      "[13,    60] loss: 0.002\n",
      "[13,    80] loss: 0.001\n",
      "[13,   100] loss: 0.001\n",
      "[13,   120] loss: 0.008\n",
      "Train Accuracy for epoch 12 = 0.9979166666666667\n",
      "Test Accuracy for epoch 12 = 0.95\n",
      "[14,    20] loss: 0.017\n",
      "[14,    40] loss: 0.001\n",
      "[14,    60] loss: 0.025\n",
      "[14,    80] loss: 0.002\n",
      "[14,   100] loss: 0.013\n",
      "[14,   120] loss: 0.008\n",
      "Train Accuracy for epoch 13 = 0.99375\n",
      "Test Accuracy for epoch 13 = 0.9\n",
      "[15,    20] loss: 0.012\n",
      "[15,    40] loss: 0.043\n",
      "[15,    60] loss: 0.012\n",
      "[15,    80] loss: 0.020\n",
      "[15,   100] loss: 0.007\n",
      "[15,   120] loss: 0.041\n",
      "Train Accuracy for epoch 14 = 0.9875\n",
      "Test Accuracy for epoch 14 = 0.9416666666666667\n",
      "[16,    20] loss: 0.005\n",
      "[16,    40] loss: 0.005\n",
      "[16,    60] loss: 0.021\n",
      "[16,    80] loss: 0.021\n",
      "[16,   100] loss: 0.028\n",
      "[16,   120] loss: 0.031\n",
      "Train Accuracy for epoch 15 = 0.9875\n",
      "Test Accuracy for epoch 15 = 0.9416666666666667\n",
      "[17,    20] loss: 0.010\n",
      "[17,    40] loss: 0.044\n",
      "[17,    60] loss: 0.013\n",
      "[17,    80] loss: 0.009\n",
      "[17,   100] loss: 0.006\n",
      "[17,   120] loss: 0.022\n",
      "Train Accuracy for epoch 16 = 0.98125\n",
      "Test Accuracy for epoch 16 = 0.9333333333333333\n",
      "[18,    20] loss: 0.005\n",
      "[18,    40] loss: 0.003\n",
      "[18,    60] loss: 0.003\n",
      "[18,    80] loss: 0.001\n",
      "[18,   100] loss: 0.002\n",
      "[18,   120] loss: 0.001\n",
      "Train Accuracy for epoch 17 = 1.0\n",
      "Test Accuracy for epoch 17 = 0.95\n",
      "[19,    20] loss: 0.001\n",
      "[19,    40] loss: 0.000\n",
      "[19,    60] loss: 0.002\n",
      "[19,    80] loss: 0.001\n",
      "[19,   100] loss: 0.000\n",
      "[19,   120] loss: 0.000\n",
      "Train Accuracy for epoch 18 = 1.0\n",
      "Test Accuracy for epoch 18 = 0.95\n",
      "[20,    20] loss: 0.001\n",
      "[20,    40] loss: 0.000\n",
      "[20,    60] loss: 0.000\n",
      "[20,    80] loss: 0.000\n",
      "[20,   100] loss: 0.000\n",
      "[20,   120] loss: 0.000\n",
      "Train Accuracy for epoch 19 = 1.0\n",
      "Test Accuracy for epoch 19 = 0.95\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_accuracy += torch.eq(predicted, labels).sum().item()\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20/len(data)))\n",
    "            running_loss = 0.0\n",
    "    net.eval()\n",
    "    test_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(testloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_accuracy += torch.eq(predicted, labels).sum().item()\n",
    "        \n",
    "    print(\"Train Accuracy for epoch \"+str(epoch)+ \" = \"+str(running_accuracy/480))\n",
    "    print(\"Test Accuracy for epoch \"+str(epoch)+ \" = \"+str(test_accuracy/120))\n",
    "    \n",
    "    # Check if test accuracy is the best so far\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        # Save the model\n",
    "        torch.save(net.state_dict(), 'furniture_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ac6279f1-abe1-4f43-948f-0953a63cf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('furniture_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4c8549d-2e9a-46be-9718-e2258235445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539690ae-8d78-4e2b-838c-c9f164626976",
   "metadata": {},
   "source": [
    "This code is used to call the web api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ad80fbb-3c0e-4772-9df6-78cee9eb90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"output\":\"Bed\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "url = \"http://127.0.0.1:5000/predict\"\n",
    "\n",
    "file_path = \"./Dataset/Bed/Aubree Queen Bed.jpg\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "payload = {\"file\": (os.path.basename(file_path), file_content)}\n",
    "headers = {}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17118891-8e0a-4102-bd88-59cd2794d7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
